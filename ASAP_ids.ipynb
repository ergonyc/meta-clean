{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASAP CRN Unique ID generation - wave 1\n",
    "\n",
    "# ASAP CRN Unique ID generation - wave 1\n",
    "\n",
    "\n",
    "Postmortem-derived Brain Sequencing Collection\n",
    "\n",
    "\n",
    "25 OCT 2023\n",
    "Andy Henrie\n",
    "\n",
    "\n",
    "### Dataset ID\n",
    "- \"ASAP_PBMSC\" to identify that it is part of the Postmortem-derived Brain Sequencing Collection\n",
    "- `ASAP_dataset_id`\n",
    "    - also need to generate a \"team_dataset_id\" (Add to CDE/DataDictionary). TeamCODE+\"one to two word descriptor\"\n",
    "\n",
    "### Team ID\n",
    "- hardcoded definitions\n",
    "- `ASAP_team_id`\n",
    "\n",
    "### Subject ID\n",
    "- unique for ASAP\n",
    "- could exist across several Teams / Datasets\n",
    "- `ASAP_subject_id`\n",
    "\n",
    "### Sample ID\n",
    "- unique for each sample\n",
    "- multiple could derive from same `ASAP_subject_id`.  \n",
    "    - multiple brain regions from a single team\n",
    "    - multiple teams from same biobank\n",
    "    - \"other\" repeated samples??\n",
    "- `ASAP_sample_id`\n",
    "- Unique ASAP_subject_id + \"sample repeat number\"\n",
    "\n",
    "\n",
    "\n",
    "###  Issues\n",
    "\n",
    "- storing \"master\" IDs for lookup:  pandas vs. json... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n lw10 python=3.10 notebook ipykernel pip pandas ijson - y && conda activate lw10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ijson\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from asap_ids import (read_meta_table, get_dtypes_dict, STUDY_PREFIX, DATASET_ID, \n",
    "                      load_id_mapper, write_id_mapper, generate_asap_sample_ids,\n",
    "                      generate_asap_subject_ids, process_meta_files)\n",
    "\n",
    "\n",
    "                       \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CDE for properly reading the team tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDE_path = Path.cwd() / \"ASAP_CDE.csv\" \n",
    "CDE = pd.read_csv(CDE_path )\n",
    "# Initialize the data types dictionary\n",
    "dtypes_dict = get_dtypes_dict(CDE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_mapper not found at /Users/ergonyc/Projects/ASAP/meta-clean/ASAP_subj_map.json\n",
      "id_mapper not found at /Users/ergonyc/Projects/ASAP/meta-clean/ASAP_samp_map.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "## test with team Lee\n",
    "export_root = Path.cwd() / \"clean/team-Lee\"\n",
    "subject_mapper_path = Path.cwd() / \"ASAP_subj_map.json\"\n",
    "sample_mapper_path = Path.cwd() / \"ASAP_samp_map.json\"\n",
    "\n",
    "try:\n",
    "    subj_id_mapper = load_id_mapper(subject_mapper_path)\n",
    "except FileNotFoundError:\n",
    "    subj_id_mapper = {}\n",
    "    print(f\"{subject_mapper_path} not found... starting from scratch\")\n",
    "\n",
    "try:\n",
    "    samp_id_mapper = load_id_mapper(sample_mapper_path)\n",
    "except FileNotFoundError:\n",
    "    samp_id_mapper = {}\n",
    "    print(f\"{sample_mapper_path} not found... starting from scratch\")\n",
    "\n",
    "if CDE_path.exists():\n",
    "    CDE = pd.read_csv(CDE_path )\n",
    "else:\n",
    "    print(f\"{CDE_path} not found... aborting\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_id_mapper = {}\n",
    "samp_id_mapper = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = export_root\n",
    "# dtypes_dict = get_dtypes_dict(CDE)\n",
    "\n",
    "# add ASAP_team_id to the STUDY and PROTOCOL tables\n",
    "study_path = table_path / \"STUDY.csv\"\n",
    "if study_path.exists():\n",
    "    study_df = read_meta_table(study_path, dtypes_dict)\n",
    "    team_id = study_df['ASAP_team_name'].str.upper().replace('-', '_')\n",
    "    study_df['ASAP_team_id'] = team_id\n",
    "    # add ASAP_dataset_id = DATASET_ID to the STUDY tables\n",
    "    study_df['ASAP_dataset_id'] = DATASET_ID\n",
    "else:\n",
    "    study_df = None\n",
    "    print(f\"{study_path} not found... aborting\")\n",
    "\n",
    "protocol_path = table_path / \"PROTOCOL.csv\"\n",
    "if protocol_path.exists():\n",
    "    protocol_df = read_meta_table(protocol_path, dtypes_dict)\n",
    "    protocol_df['ASAP_team_id'] = team_id\n",
    "else:\n",
    "    protocol_df = None\n",
    "    print(f\"{protocol_path} not found... aborting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add ASAP_subject_id to the SUBJECT tables\n",
    "subject_path = table_path / \"SUBJECT.csv\"\n",
    "if subject_path.exists():\n",
    "    subject_df = read_meta_table(subject_path, dtypes_dict)\n",
    "    subj_id_mapper, subject_df, n = generate_asap_subject_ids(subj_id_mapper, subject_df)\n",
    "    # add ASAP_dataset_id = DATASET_ID to the SUBJECT tables\n",
    "    subject_df['ASAP_dataset_id'] = DATASET_ID\n",
    "else:\n",
    "    subject_df = None\n",
    "    print(f\"{subject_path} not found... aborting\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HC_1225': 'ASAP_PMBDS_000001',\n",
       " 'HC_0602': 'ASAP_PMBDS_000002',\n",
       " 'PD_0009': 'ASAP_PMBDS_000003',\n",
       " 'PD_1921': 'ASAP_PMBDS_000004',\n",
       " 'PD_2058': 'ASAP_PMBDS_000005',\n",
       " 'PD_1441': 'ASAP_PMBDS_000006',\n",
       " 'PD_1344': 'ASAP_PMBDS_000007',\n",
       " 'HC_1939': 'ASAP_PMBDS_000008',\n",
       " 'HC_1308': 'ASAP_PMBDS_000009',\n",
       " 'HC_1862': 'ASAP_PMBDS_000010',\n",
       " 'HC_1864': 'ASAP_PMBDS_000011',\n",
       " 'HC_2057': 'ASAP_PMBDS_000012',\n",
       " 'HC_2061': 'ASAP_PMBDS_000013',\n",
       " 'HC_2062': 'ASAP_PMBDS_000014',\n",
       " 'HC_2067': 'ASAP_PMBDS_000015',\n",
       " 'PD_0348': 'ASAP_PMBDS_000016',\n",
       " 'PD_0413': 'ASAP_PMBDS_000017',\n",
       " 'PD_1312': 'ASAP_PMBDS_000018',\n",
       " 'PD_1317': 'ASAP_PMBDS_000019',\n",
       " 'PD_1504': 'ASAP_PMBDS_000020',\n",
       " 'PD_1858': 'ASAP_PMBDS_000021',\n",
       " 'PD_1902': 'ASAP_PMBDS_000022',\n",
       " 'PD_1973': 'ASAP_PMBDS_000023',\n",
       " 'PD_2005': 'ASAP_PMBDS_000024',\n",
       " 'PD_2038': 'ASAP_PMBDS_000025'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_id_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASAP_subject_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>source_subject_id</th>\n",
       "      <th>biobank_name</th>\n",
       "      <th>organism</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_at_collection</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>duration_pmi</th>\n",
       "      <th>primary_diagnosis</th>\n",
       "      <th>primary_diagnosis_text</th>\n",
       "      <th>uid_idx</th>\n",
       "      <th>uid_idx_cumcount</th>\n",
       "      <th>ASAP_dataset_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASAP_PMBDS_000001</td>\n",
       "      <td>HC_1225</td>\n",
       "      <td>12-25</td>\n",
       "      <td>Banner Sun Health Research Institute</td>\n",
       "      <td>Human</td>\n",
       "      <td>Male</td>\n",
       "      <td>80</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>3.50</td>\n",
       "      <td>No PD nor other neurological disorder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ASAP_PMBDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASAP_PMBDS_000002</td>\n",
       "      <td>HC_0602</td>\n",
       "      <td>06-02</td>\n",
       "      <td>Banner Sun Health Research Institute</td>\n",
       "      <td>Human</td>\n",
       "      <td>Male</td>\n",
       "      <td>84</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>2.66</td>\n",
       "      <td>Other neurological disorder</td>\n",
       "      <td>Mild Cognitive Impairment</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>ASAP_PMBDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASAP_PMBDS_000003</td>\n",
       "      <td>PD_0009</td>\n",
       "      <td>00-09</td>\n",
       "      <td>Banner Sun Health Research Institute</td>\n",
       "      <td>Human</td>\n",
       "      <td>Male</td>\n",
       "      <td>64</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Idiopathic PD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>ASAP_PMBDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASAP_PMBDS_000004</td>\n",
       "      <td>PD_1921</td>\n",
       "      <td>19-21</td>\n",
       "      <td>Banner Sun Health Research Institute</td>\n",
       "      <td>Human</td>\n",
       "      <td>Male</td>\n",
       "      <td>82</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>3.93</td>\n",
       "      <td>Idiopathic PD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>ASAP_PMBDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASAP_PMBDS_000005</td>\n",
       "      <td>PD_2058</td>\n",
       "      <td>20-58</td>\n",
       "      <td>Banner Sun Health Research Institute</td>\n",
       "      <td>Human</td>\n",
       "      <td>Male</td>\n",
       "      <td>87</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>3.17</td>\n",
       "      <td>Idiopathic PD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>ASAP_PMBDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ASAP_subject_id subject_id source_subject_id  \\\n",
       "0  ASAP_PMBDS_000001    HC_1225             12-25   \n",
       "1  ASAP_PMBDS_000002    HC_0602             06-02   \n",
       "2  ASAP_PMBDS_000003    PD_0009             00-09   \n",
       "3  ASAP_PMBDS_000004    PD_1921             19-21   \n",
       "4  ASAP_PMBDS_000005    PD_2058             20-58   \n",
       "\n",
       "                           biobank_name organism   sex  age_at_collection  \\\n",
       "0  Banner Sun Health Research Institute    Human  Male                 80   \n",
       "1  Banner Sun Health Research Institute    Human  Male                 84   \n",
       "2  Banner Sun Health Research Institute    Human  Male                 64   \n",
       "3  Banner Sun Health Research Institute    Human  Male                 82   \n",
       "4  Banner Sun Health Research Institute    Human  Male                 87   \n",
       "\n",
       "    race     ethnicity  duration_pmi                      primary_diagnosis  \\\n",
       "0  White  Not Reported          3.50  No PD nor other neurological disorder   \n",
       "1  White  Not Reported          2.66            Other neurological disorder   \n",
       "2  White  Not Reported          4.00                          Idiopathic PD   \n",
       "3  White  Not Reported          3.93                          Idiopathic PD   \n",
       "4  White  Not Reported          3.17                          Idiopathic PD   \n",
       "\n",
       "      primary_diagnosis_text  uid_idx  uid_idx_cumcount ASAP_dataset_id  \n",
       "0                        NaN        1                 1      ASAP_PMBDS  \n",
       "1  Mild Cognitive Impairment        2                 1      ASAP_PMBDS  \n",
       "2                        NaN        3                 1      ASAP_PMBDS  \n",
       "3                        NaN        4                 1      ASAP_PMBDS  \n",
       "4                        NaN        5                 1      ASAP_PMBDS  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add ASAP_sample_id and ASAP_dataset_id to the SAMPLE tables\n",
    "sample_path = table_path / \"SAMPLE.csv\"\n",
    "if sample_path.exists():\n",
    "    sample_df = read_meta_table(sample_path, dtypes_dict)\n",
    "    subj_id_mapper, sample_df = generate_asap_sample_ids(subj_id_mapper, sample_df, n, samp_id_mapper)\n",
    "    sample_df['ASAP_dataset_id'] = DATASET_ID\n",
    "else:\n",
    "    sample_df = None\n",
    "    print(f\"{sample_path} not found... aborting\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HC_1225': 'ASAP_PMBDS_000001',\n",
       " 'HC_0602': 'ASAP_PMBDS_000002',\n",
       " 'PD_0009': 'ASAP_PMBDS_000003',\n",
       " 'PD_1921': 'ASAP_PMBDS_000004',\n",
       " 'PD_2058': 'ASAP_PMBDS_000005',\n",
       " 'PD_1441': 'ASAP_PMBDS_000006',\n",
       " 'PD_1344': 'ASAP_PMBDS_000007',\n",
       " 'HC_1939': 'ASAP_PMBDS_000008',\n",
       " 'HC_1308': 'ASAP_PMBDS_000009',\n",
       " 'HC_1862': 'ASAP_PMBDS_000010',\n",
       " 'HC_1864': 'ASAP_PMBDS_000011',\n",
       " 'HC_2057': 'ASAP_PMBDS_000012',\n",
       " 'HC_2061': 'ASAP_PMBDS_000013',\n",
       " 'HC_2062': 'ASAP_PMBDS_000014',\n",
       " 'HC_2067': 'ASAP_PMBDS_000015',\n",
       " 'PD_0348': 'ASAP_PMBDS_000016',\n",
       " 'PD_0413': 'ASAP_PMBDS_000017',\n",
       " 'PD_1312': 'ASAP_PMBDS_000018',\n",
       " 'PD_1317': 'ASAP_PMBDS_000019',\n",
       " 'PD_1504': 'ASAP_PMBDS_000020',\n",
       " 'PD_1858': 'ASAP_PMBDS_000021',\n",
       " 'PD_1902': 'ASAP_PMBDS_000022',\n",
       " 'PD_1973': 'ASAP_PMBDS_000023',\n",
       " 'PD_2005': 'ASAP_PMBDS_000024',\n",
       " 'PD_2038': 'ASAP_PMBDS_000025'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_id_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add ASAP_sample_id to the CLINPATH tables\n",
    "clinpath_path = table_path / \"CLINPATH.csv\"\n",
    "if clinpath_path.exists():\n",
    "    clinpath_df = read_meta_table(clinpath_path, dtypes_dict)\n",
    "    clinpath_df['ASAP_sample_id'] = clinpath_df['sample_id'].map(samp_id_mapper)\n",
    "\n",
    "# once we update the CDE so CLINPATH has subject level data we can add this\n",
    "# # add ASAP_subject_id to the CLINPATH tables\n",
    "# clinpath_path = table_path / \"CLINPATH.csv\"\n",
    "# if clinpath_path.exists():\n",
    "#     clinpath_df = read_meta_table(clinpath_path, dtypes_dict)\n",
    "\n",
    "#     clinpath_df['ASAP_subject_id'] = clinpath_df['subject_id'].map(id_mapper)\n",
    "\n",
    "# export updated tables\n",
    "asap_tables_path = Path.cwd() / \"ASAP_tables\"\n",
    "if  not asap_tables_path.exists():\n",
    "    asap_tables_path.mkdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if study_path.exists():\n",
    "    study_df.to_csv(asap_tables_path / study_path.name)\n",
    "if protocol_path.exists():\n",
    "    protocol_df.to_csv(asap_tables_path / protocol_path.name)\n",
    "if subject_path.exists():\n",
    "    subject_df.to_csv(asap_tables_path / subject_path.name)\n",
    "if sample_path.exists():\n",
    "    sample_df.to_csv(asap_tables_path / sample_path.name)\n",
    "if clinpath_path.exists():\n",
    "    clinpath_df.to_csv(asap_tables_path / clinpath_path.name)\n",
    "\n",
    "\n",
    "# write the updated id_mapper to file\n",
    "write_id_mapper(subj_id_mapper, subject_mapper_path)\n",
    "write_id_mapper(samp_id_mapper, sample_mapper_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_TEAM_ids = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STUDY: Postmortem-derived Brain Sequencing Collection (PMBDS) \n",
    "\n",
    "All ASAP_dataset_id, and ASAP_subject_id here will start with \"ASAP_PMBDS_\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_PREFIX = \"ASAP_PMBDS_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# function to assign the study prefix to the id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ASAP_team_id`\n",
    "\n",
    "On meta-data ingest, add this to:\n",
    "- STUDY, PROTOCOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names = [\"lee\", \"hafler\", \"hardy\", \"jakobsson\", \"sherzer\",\"sulzer\", \"voet\",\"wood\"]\n",
    "[x.upper() for x in team_names]\n",
    "MASTER_TEAM_ids['team_name'] = team_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_codes = [\"LEE\", \"HAF\", \"HAR\", \"JAK\", \"SHE\", \"SUL\", \"VOE\", \"WOO\"]\n",
    "\n",
    "MASTER_TEAM_ids['team_code'] = team_codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_TEAM_ids['ASAP_team_id'] = \"TEAM_\" + MASTER_TEAM_ids['team_name'].str.upper() \n",
    "MASTER_TEAM_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ASAP_dataset_id`\n",
    "\n",
    "This compares with the GP2 \"study code\".\n",
    "\n",
    "This is done by hand for now. On meta-data ingest, add this (?) to:\n",
    "- STUDY, PROTOCOL, SAMPLE\n",
    "\n",
    "\n",
    "\n",
    "Currently we have:\n",
    "- Team Lee \n",
    "- Team Hardy\n",
    "- Team Hafler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MASTER_DATASET_IDs = pd.DataFrame()\n",
    "\n",
    "MASTER_DATASET_IDs['ASAP_dataset_id'] = \"ASAP_PMBDS\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ASAP_subject_id`\n",
    "\n",
    "\n",
    "### Subject ID\n",
    "- unique for ASAP\n",
    "- could exist across several Teams / Datasets\n",
    "- `ASAP_subject_id`\n",
    "\n",
    "\n",
    "On meta-data ingest, add this to:\n",
    "- SUBJECT\n",
    "\n",
    "\"ASAP_XXXXXXX\"\n",
    "\n",
    "Team Lee:  \n",
    "\n",
    "Team Hardy:\n",
    "\n",
    "Team Hafler:\n",
    "\n",
    "\n",
    "\n",
    "We need to define a function that creates the _master_archive_ (if it doesn't exist), and assigns  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_SUBJECT_IDs = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_root = Path.cwd() / \"clean/team-Lee\"\n",
    "# make sure cleaned files are correct\n",
    "\n",
    "\n",
    "SUBJECT = read_meta_table(f\"{export_root}/SUBJECT.csv\", dtypes_dict)\n",
    "CLINPATH = read_meta_table(f\"{export_root}/CLINPATH.csv\", dtypes_dict)\n",
    "STUDY = read_meta_table(f\"{export_root}/STUDY.csv\", dtypes_dict)\n",
    "PROTOCOL = read_meta_table(f\"{export_root}/PROTOCOL.csv\", dtypes_dict)\n",
    "SAMPLE = read_meta_table(f\"{export_root}/SAMPLE.csv\", dtypes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = SUBJECT\n",
    "\n",
    "\n",
    "def generate_asap_subject_ids(subj_id_mapper, subject_df):\n",
    "    \"\"\"\n",
    "    generate new unique_ids for new subject_ids in subject_df table, \n",
    "    update the id_mapper with the new ids from the data table\n",
    "\n",
    "    return the updated id_mapper\n",
    "    \"\"\"\n",
    "    # extract the max value of the mapper's third (last) section ([2] or [-1]) to get our n\n",
    "    if bool(subj_id_mapper):\n",
    "        n = max([int(v.split(\"_\")[2]) for v in subj_id_mapper.values() if v]) + 1\n",
    "    else:\n",
    "        n = 1\n",
    "\n",
    "    df_nodups_wids = subject_df.copy()\n",
    "    # might want to use 'source_subject_id' instead of 'subject_id' since we want to find matches across teams\n",
    "    # shouldn't actually matter but logically cleaner\n",
    "    uids = [str(id) for id in df_nodups_wids['subject_id'].unique()]\n",
    "    mapid = {}\n",
    "    for uid in uids:\n",
    "        mapid[uid]= n\n",
    "        n += 1\n",
    "\n",
    "    df_nodups_wids['uid_idx'] = df_nodups_wids['subject_id'].map(mapid)\n",
    "    # make a new column with the ASAP_subject_id\n",
    "    # and insert it at the beginning of the dataframe\n",
    "    ASAP_subject_id = [f'{STUDY_PREFIX}{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "    df_nodups_wids.insert(0, 'ASAP_subject_id', ASAP_subject_id)\n",
    "    # df_nodups_wids['ASAP_subject_id'] = [f'{STUDY_PREFIX}{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "    df_nodups_wids['uid_idx_cumcount'] = df_nodups_wids.groupby('ASAP_subject_id').cumcount() + 1\n",
    "    asap_id_mapper = dict(zip(df_nodups_wids['subject_id'], df_nodups_wids['ASAP_subject_id']))\n",
    "\n",
    "    subj_id_mapper.update(asap_id_mapper)\n",
    "    \n",
    "    return subj_id_mapper, df_nodups_wids, n\n",
    "\n",
    "\n",
    "\n",
    "def generate_asap_sample_ids(subj_id_mapper, sample_df, n, samp_id_mapper):\n",
    "    \"\"\"\n",
    "    generate new unique_ids for new sample_ids in sample_df table, \n",
    "    update the id_mapper with the new ids from the data table\n",
    "\n",
    "\n",
    "    return the updated id_mapper\n",
    "    \"\"\"\n",
    "    # could pass subj_id_mapper as a parameter instead of n.  e.g.\n",
    "    # if bool(subj_id_mapper):\n",
    "    #     n = max([int(v.split(\"_\")[2]) for v in subj_id_mapper.values() if v]) + 1\n",
    "    # else:\n",
    "    #     n = 1\n",
    "    \n",
    "    # since the current SAMPLE tables can have multipl sample_ids lets drop duplciates, with the caveat of replciates\n",
    "    df_nodups = sample_df.drop_duplicates(subset=['sample_id','replicate'])\n",
    "    \n",
    "    # \n",
    "    uniq_subj = df_nodups.subject_id.unique()\n",
    "\n",
    "    dupids_mapper = dict(zip(uniq_subj,\n",
    "                        [num+n for num in range(len(uniq_subj))] ))\n",
    "\n",
    "    df_dup_chunks = []\n",
    "    for subj_id, samp_n in dupids_mapper.items():\n",
    "        df_dups_subset = df_nodups[df_nodups.subject_id==subj_id].copy()\n",
    "        asap_id = subj_id_mapper[subj_id]\n",
    "        df_dups_subset['asap_sample'] = [f'{STUDY_PREFIX}{asap_id}_{samp_n:06}' for i in range(df_dups_subset.shape[0])]\n",
    "        df_dups_subset['samp_rep_no'] = ['s'+str(i+1) for i in range(df_dups_subset.shape[0])]\n",
    "        # make a new column with the asap_sample_id\n",
    "        # and insert it at the beginning of the dataframe\n",
    "        ASAP_sample_id = df_dups_subset['asap_sample'] + '_' + df_dups_subset['samp_rep_no']\n",
    "        df_dups_subset.insert(0, 'ASAP_sample_id', ASAP_sample_id)\n",
    "\n",
    "        df_dup_chunks.append(df_dups_subset)\n",
    "    df_dups_wids = pd.concat(df_dup_chunks)\n",
    "\n",
    "\n",
    "\n",
    "    id_mapper = dict(zip(df_dups_wids.sample_id,\n",
    "                        df_dups_wids.ASAP_sample_id))\n",
    "    out_df = sample_df.copy()\n",
    "    out_df['ASAP_sample_id'] = out_df['sample_id'].map(id_mapper)\n",
    "\n",
    "    samp_id_mapper.update(id_mapper)\n",
    "    return subj_id_mapper, out_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subj_id_mapper = {}\n",
    "samp_id_mapper = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_subj_id_mapper, ud_subject_df, n = generate_asap_subject_ids(subj_id_mapper, subject_df)\n",
    "ud_subj_id_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_subject_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = SAMPLE.copy()\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ud_subj_id_mapper, ud_sample_df = generate_asap_sample_ids(ud_subj_id_mapper, SAMPLE, n, samp_id_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_root = Path.cwd() / \"clean/team-Hardy\"\n",
    "# make sure cleaned files are correct\n",
    "\n",
    "\n",
    "SUBJECT = read_meta_table(f\"{export_root}/SUBJECT.csv\", dtypes_dict)\n",
    "CLINPATH = read_meta_table(f\"{export_root}/CLINPATH.csv\", dtypes_dict)\n",
    "STUDY = read_meta_table(f\"{export_root}/STUDY.csv\", dtypes_dict)\n",
    "PROTOCOL = read_meta_table(f\"{export_root}/PROTOCOL.csv\", dtypes_dict)\n",
    "SAMPLE = read_meta_table(f\"{export_root}/SAMPLE.csv\", dtypes_dict)\n",
    "\n",
    "subject_df = SUBJECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_subj_id_mapper, ud_subject_df, n = generate_asap_subject_ids(ud_subj_id_mapper, subject_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_subj_id_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set starting n = 3\n",
    "n = 3\n",
    "\n",
    "df_nodups_wids = df_nodups.copy()\n",
    "uids = [str(id) for id in df_nodups['source_subject_id'].unique()]\n",
    "mapid = {}\n",
    "for uid in uids:\n",
    "    mapid[uid]= n\n",
    "    n += 1\n",
    "\n",
    "df_nodups_wids['uid_idx'] = df_nodups_wids['source_subject_id'].map(mapid)\n",
    "df_nodups_wids['ASAP_subject_id'] = [f'{STUDY_PREFIX}{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "df_nodups_wids['uid_idx_cumcount'] = df_nodups_wids.groupby('ASAP_subject_id').cumcount() + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "asap_id_mapper = dict(zip(df_nodups_wids['source_subject_id'], df_nodups_wids['ASAP_subject_id']))\n",
    "\n",
    "ASAPsubject_df = df_nodups_wids.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([int(v.split(\"_\")[2]) for v in asap_id_mapper.values()])+1\n",
    "asap_id_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = SAMPLE.copy()\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df.duplicated(keep=False, subset=['sample_id'])\n",
    "# ~sample_df.duplicated(keep=False, subset=['sample_id'])\n",
    "df_nodups = sample_df.drop_duplicates(subset=['sample_id','replicate'])\n",
    "df_nodups.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# #  could do this  easier with groupby:\n",
    "# df_nodups_wids['uid_idx'] = df_nodups_wids['source_subject_id'].map(mapid)\n",
    "# df_nodups_wids['ASAP_subject_id'] = [f'{STUDY_PREFIX}{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "# df_nodups_wids['uid_idx_cumcount'] = df_nodups_wids.groupby('ASAP_subject_id').cumcount() + 1\n",
    "\n",
    "\n",
    "# \n",
    "uniq_subj = df_nodups.subject_id.unique()\n",
    "\n",
    "dupids_mapper = dict(zip(uniq_subj,\n",
    "                    [num+n for num in range(len(uniq_subj))] ))\n",
    "\n",
    "df_dup_chunks = []\n",
    "for subj_id, samp_n in dupids_mapper.items():\n",
    "    df_dups_subset = df_nodups[df_nodups.subject_id==subj_id].copy()\n",
    "    df_dups_subset['asap_sample'] = [f'{STUDY_PREFIX}{subj_id}_{samp_n:06}' for i in range(df_dups_subset.shape[0])]\n",
    "    df_dups_subset['samp_rep_no'] = ['s'+str(i+1) for i in range(df_dups_subset.shape[0])]\n",
    "    df_dups_subset['ASAP_sample_id'] = df_dups_subset['asap_sample'] + '_' + df_dups_subset['samp_rep_no']\n",
    "    df_dup_chunks.append(df_dups_subset)\n",
    "df_dups_wids = pd.concat(df_dup_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_id_mapper = dict(zip(df_dups_wids.sample_id,\n",
    "                    df_dups_wids.ASAP_sample_id))\n",
    "\n",
    "\n",
    "sample_df['ASAP_sample_id'] = sample_df['sample_id'].map(sample_id_mapper)\n",
    "\n",
    "ASAPsample_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sorted = sample_df.sort_values('sample_id').reset_index(drop = True).copy()\n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sorted['ASAP_subject_id'] = df_sorted['subject_id'].map(asap_id_mapper)\n",
    "# df_sorted['ASAP_subject_id'] = [f'{STUDY_PREFIX}{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "# df_sorted['uid_idx_cumcount'] = df_sorted.groupby('sample_id').cumcount() + 1\n",
    "\n",
    "data_duplicated = pd.merge(df_sorted, ASAPsubject_df, on=['ASAP_subject_id'], how='right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_duplicated[['ASAP_subject_id','sample_id', 'source_sample_id', 'subject_id_x', 'replicate',\n",
    "       'replicate_count', 'repeated_sample', 'batch', \n",
    "       'uid_idx_cumcount_x', 'subject_id_y', 'source_subject_id',\n",
    "      'uid_idx', 'uid_idx_cumcount_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sorted['asap_samp_id'] = df_sorted.ASAP_subject_id + '_s' + df_nodups_wids.uid_idx_cumcount.astype('str')\n",
    "df_nodups_wids['sample_rep_num'] = 's' + df_nodups_wids.uid_idx_cumcount.astype('str')\n",
    "df_nodups_wids.drop(['uid_idx','uid_idx_cumcount'], axis = 1, inplace = True)\n",
    "df_sorted['uid_idx_cumcount'] = df_sorted.groupby('sample_id').cumcount() + 1\n",
    "\n",
    "df_sorted[['sample_id', 'subject_id', 'ASAP_subject_id','uid_idx_cumcount']].head()\n",
    "\n",
    "ASAPsample_df = df_sorted.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set starting n = 3\n",
    "n = 3\n",
    "\n",
    "\n",
    "uids = [str(id) for id in df_nodups['sample_id'].unique()]\n",
    "mapid = {}\n",
    "for uid in uids:\n",
    "    mapid[uid]= n\n",
    "    n += 1\n",
    "\n",
    "\n",
    "df_nodups_wids = df_nodups.copy()\n",
    "df_nodups_wids['uid_idx'] = df_nodups_wids['sample_id'].map(mapid)\n",
    "df_nodups_wids['ASAP_subject_id'] = [f'{STUDY_PREFIX}{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "df_nodups_wids['uid_idx_cumcount'] = df_nodups_wids.groupby('ASAP_subject_id').cumcount() + 1\n",
    "df_nodups_wids['GP2sampleID'] = df_nodups_wids.ASAP_subject_id + '_s' + df_nodups_wids.uid_idx_cumcount.astype('str')\n",
    "df_nodups_wids['SampleRepNo'] = 's' + df_nodups_wids.uid_idx_cumcount.astype('str')\n",
    "df_nodups_wids.drop(['uid_idx','uid_idx_cumcount'], axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_tracker = generategp2ids.master_key(studies = studynames)\n",
    "\n",
    "df['GP2sampleID'] = None\n",
    "\n",
    "tracking = 'ASAP_sample_id' in MASTER_SUBJECT_IDs.columns\n",
    "\n",
    "\n",
    "if tracking:\n",
    "    # check for DUPLICATED IDS\n",
    "\n",
    "    # df_subset = df_subset.reset_index()\n",
    "    # data_duplicated = pd.merge(df_subset, study_tracker_df, on=['clinical_id'], how='inner')\n",
    "    # df_subset = df_subset.set_index('index')\n",
    "    # df_subset.index.name = None\n",
    "\n",
    "    if data_duplicated.shape[0]>0:\n",
    "        new_clinicaldups = True\n",
    "        newids_clinicaldups = data_duplicated.groupby('clinical_id')\\\n",
    "                                        .apply(lambda x: generategp2ids.assign_unique_gp2clinicalids(df_subset,x))\n",
    "\n",
    "        if newids_clinicaldups.shape[0]>0:\n",
    "            newids_clinicaldups = newids_clinicaldups.reset_index(drop=True)[['study','clinical_id','sample_id','GP2sampleID']]\n",
    "            log_new.append(newids_clinicaldups)\n",
    "    else:\n",
    "        new_clinicaldups = False\n",
    "        newids_clinicaldups = pd.DataFrame()\n",
    "\n",
    "    # GET GP2 IDs METADATA for new CLINICAL-SAMPLE ID pairs\n",
    "    df_newids = df_subset[df_subset['GP2sampleID'].isnull()].reset_index(drop = True).copy()\n",
    "    if not df_newids.empty: # Get new GP2 IDs\n",
    "        df_wids = df_subset[~df_subset['GP2sampleID'].isnull()].reset_index(drop = True).copy()\n",
    "        df_wids['GP2ID'] = df_wids['GP2sampleID'].apply(lambda x: (\"_\").join(x.split(\"_\")[:-1]))\n",
    "        df_wids['SampleRepNo'] = df_wids['GP2sampleID'].apply(lambda x: x.split(\"_\")[-1])#.replace(\"s\",\"\"))\n",
    "\n",
    "        n=int(max(study_tracker_df['master_GP2sampleID'].to_list()).split(\"_\")[1])+1\n",
    "        df_newids = generategp2ids.getgp2idsv2(df_newids, n, study)\n",
    "        df_subset = pd.concat([df_newids, df_wids], axis = 0)\n",
    "        study_subsets.append(df_subset)\n",
    "        log_new.append(df_newids[['study','clinical_id','sample_id','GP2sampleID']])\n",
    "        \n",
    "    else: # TO CONSIDER THE CASE IN WHICH WE ONLY HAD DUPLICATE IDS MAPPED ON THE MASTER FILE\n",
    "        df_subset['GP2ID'] = df_subset['GP2sampleID'].apply(lambda x: (\"_\").join(x.split(\"_\")[:-1]))\n",
    "        df_subset['SampleRepNo'] = df_subset['GP2sampleID'].apply(lambda x: x.split(\"_\")[-1])#.replace(\"s\",\"\"))\n",
    "        study_subsets.append(df_subset)\n",
    "\n",
    "# Brand new data. create IDs and export tracking\n",
    "else:\n",
    "    study = study\n",
    "    new_clinicaldups = False # Duplicates from master key json are treated differently to brand new data\n",
    "    n = 1\n",
    "    df_newids = generategp2ids.getgp2idsv2(df_subset, n, study)\n",
    "    study_subsets.append(df_newids)\n",
    "\n",
    "\n",
    "# CODE TO UPDATE THE GET FILE WE WILL USE TO UPDATE MASTER JSON\n",
    "if (new_clinicaldups) and (newids_clinicaldups.shape[0]>0):\n",
    "    tmp = pd.concat([df_newids[['study','clinical_id','sample_id','GP2sampleID']], newids_clinicaldups])\n",
    "    tmp['master_value'] = list(zip(tmp['GP2sampleID'],\n",
    "                                    tmp['clinical_id']))\n",
    "    ids_log = tmp.groupby('study').apply(lambda x: dict(zip(x['sample_id'],\n",
    "                                                            x['master_value']))).to_dict()\n",
    "else:\n",
    "    df_update_master = df_newids.copy()\n",
    "    df_update_master['master_value'] = list(zip(df_update_master['GP2sampleID'],\n",
    "                                            df_update_master['clinical_id']))\n",
    "    ids_log = df_update_master.groupby('study').apply(lambda x: dict(zip(x['sample_id'],\n",
    "                                                                    x['master_value']))).to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ASAP_sample_id`\n",
    "\n",
    "- unique for each sample\n",
    "- multiple could derive from same `ASAP_subject_id`\n",
    "- `ASAP_sample_id`\n",
    "- Unique ASAP_subject_id + \"sample repeat number\"\n",
    "\n",
    "\n",
    "On meta-data ingest, add this to:\n",
    "- SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_SAMPLE_IDs = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . ASAP_sample_id, ASAP_dataset_id \n",
    "# ASAP_sample_id: “ASAP generated unique sample ID”\n",
    "# “ASAP_” + incrementing 6 digit number.  e.g. “ASAP_000001”\n",
    "# maybe add a sample number (e.g. for replicates)\n",
    "# maybe add a 3 digit team code (e.g. “HAF”)\n",
    "# ASAP_dataset_id:  “ASAP generated unique dataset ID”\n",
    "# “{team_name}_” + one or two word description combined with “_”\n",
    "# caveat: \n",
    "# mechanism where to access “master” list of ASAP_sample_id’s to test for collisions.  POSTGRES?\n",
    "# need to add entries for these to the CDE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean each Team Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "export_root = Path.cwd() / \"clean/team-Lee\"\n",
    "# make sure cleaned files are correct\n",
    "\n",
    "\n",
    "SUBJECT = read_meta_table(f\"{export_root}/SUBJECT.csv\", dtypes_dict)\n",
    "CLINPATH = read_meta_table(f\"{export_root}/CLINPATH.csv\", dtypes_dict)\n",
    "STUDY = read_meta_table(f\"{export_root}/STUDY.csv\", dtypes_dict)\n",
    "PROTOCOL = read_meta_table(f\"{export_root}/PROTOCOL.csv\", dtypes_dict)\n",
    "SAMPLE = read_meta_table(f\"{export_root}/SAMPLE.csv\", dtypes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SUBJECT['source_subject_id'].unique()),len(SUBJECT['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE['sample_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT[['subject_id', 'source_subject_id', 'biobank_name','primary_diagnosis']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CLINPATH[['sample_id', 'source_sample_id', 'GP2_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE[['sample_id', 'source_sample_id', 'subject_id', 'replicate',\n",
    "       'replicate_count', 'repeated_sample', 'batch', 'tissue','donor_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Hafler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to seurat Object\n",
    "data_path = Path.home() / (\"Projects/ASAP\")\n",
    "metadata_path = data_path / \"team-hafler/metadata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_root = Path.cwd() / \"clean/team-Hafler\"\n",
    "# make sure cleaned files are correct\n",
    "\n",
    "SUBJECT = read_meta_table(f\"{export_root}/SUBJECT.csv\", dtypes_dict)\n",
    "CLINPATH = read_meta_table(f\"{export_root}/CLINPATH.csv\", dtypes_dict)\n",
    "STUDY = read_meta_table(f\"{export_root}/STUDY.csv\", dtypes_dict)\n",
    "PROTOCOL = read_meta_table(f\"{export_root}/PROTOCOL.csv\", dtypes_dict)\n",
    "SAMPLE = read_meta_table(f\"{export_root}/SAMPLE.csv\", dtypes_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Hardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert \n",
    "data_path = Path.home() / (\"Projects/ASAP/team-hardy\")\n",
    "metadata_path = data_path / \"metadata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_root = Path.cwd() / \"clean/team-Hardy\"\n",
    "# make sure cleaned files are correct\n",
    "\n",
    "SUBJECT = read_meta_table(f\"{export_root}/SUBJECT.csv\", dtypes_dict)\n",
    "CLINPATH = read_meta_table(f\"{export_root}/CLINPATH.csv\", dtypes_dict)\n",
    "STUDY = read_meta_table(f\"{export_root}/STUDY.csv\", dtypes_dict)\n",
    "PROTOCOL = read_meta_table(f\"{export_root}/PROTOCOL.csv\", dtypes_dict)\n",
    "SAMPLE = read_meta_table(f\"{export_root}/SAMPLE.csv\", dtypes_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basically hold the list of the GP2ID and the original clinical ID pairs + how many samples are in the GP2 (s1 only or s1, s2,...) for all GP2 submitted individuals. It takes the sample manifest, scan the clinical ID to check if this is the additional submission of those already in the GP2 or not and then if its new, give new GP2ID and GP2sampleID. If the clinical_id is already existing in the GP2 then only provide GP2sampleID (GP2ID_sX+1). Also it errors if the original sample ID submitted is equal to the one in the list. (No duplication of sample ID from the same cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    uids = [str(id) for id in df_nodups['sample_id'].unique()]\n",
    "    mapid = {}\n",
    "    for uid in uids:\n",
    "        mapid[uid]= n\n",
    "        n += 1\n",
    "\n",
    "\n",
    "def master_keyv2(studies):\n",
    "    # ACCESS MASTERGP2IDS_JSON IN GP2 BUCKET\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket('eu-samplemanifest')\n",
    "    blob = bucket.blob('IDSTRACKER/GP2IDSMAPPER.json')\n",
    "    \n",
    "    ids_tracker = {}\n",
    "    with blob.open(\"r\") as f:\n",
    "        for k, v in ijson.kvitems(f, ''):\n",
    "            if k in studies:\n",
    "                ids_tracker.update({k:v})\n",
    "    \n",
    "    return(ids_tracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getgp2idsv2(dfproc, n, study_code):\n",
    "    df_dups = dfproc[dfproc.duplicated(keep=False, subset=['sample_id'])].sort_values('sample_id').reset_index(drop = True).copy()\n",
    "    if df_dups.shape[0]>0:\n",
    "        dupids_mapper = dict(zip(df_dups.clinical_id.unique(),\n",
    "                            [num+n for num in range(len(df_dups.clinical_id.unique()))]))\n",
    "        \n",
    "        df_dup_chunks = []\n",
    "        for clin_id, gp2id in dupids_mapper.items():\n",
    "            df_dups_subset = df_dups[df_dups.clinical_id==clin_id].copy()\n",
    "            df_dups_subset['GP2ID'] = [f'{study_code}_{gp2id:06}' for i in range(df_dups_subset.shape[0])]\n",
    "            df_dups_subset['SampleRepNo'] = ['s'+str(i+1) for i in range(df_dups_subset.shape[0])]\n",
    "            df_dups_subset['GP2sampleID'] = df_dups_subset['GP2ID'] + '_' + df_dups_subset['SampleRepNo']\n",
    "            df_dup_chunks.append(df_dups_subset)\n",
    "        df_dups_wids = pd.concat(df_dup_chunks)\n",
    "\n",
    "    df_nodups = dfproc[~dfproc.duplicated(keep=False, subset=['clinical_id'])].sort_values('clinical_id').reset_index(drop = True).copy()\n",
    "\n",
    "    if df_dups.shape[0]>0:\n",
    "        n =  len(list(dupids_mapper.values())) + n\n",
    "    else:\n",
    "        n = n\n",
    "\n",
    "    uids = [str(id) for id in df_nodups['sample_id'].unique()]\n",
    "    mapid = {}\n",
    "    for uid in uids:\n",
    "        mapid[uid]= n\n",
    "        n += 1\n",
    "    df_nodups_wids = df_nodups.copy()\n",
    "    df_nodups_wids['uid_idx'] = df_nodups_wids['sample_id'].map(mapid)\n",
    "    df_nodups_wids['GP2ID'] = [f'{study_code}_{i:06}' for i in df_nodups_wids.uid_idx]\n",
    "    df_nodups_wids['uid_idx_cumcount'] = df_nodups_wids.groupby('GP2ID').cumcount() + 1\n",
    "    df_nodups_wids['GP2sampleID'] = df_nodups_wids.GP2ID + '_s' + df_nodups_wids.uid_idx_cumcount.astype('str')\n",
    "    df_nodups_wids['SampleRepNo'] = 's' + df_nodups_wids.uid_idx_cumcount.astype('str')\n",
    "    df_nodups_wids.drop(['uid_idx','uid_idx_cumcount'], axis = 1, inplace = True)\n",
    "\n",
    "    if df_dups.shape[0]>0:\n",
    "        df_newids = pd.concat([df_dups_wids, df_nodups_wids])\n",
    "    else:\n",
    "        df_newids = df_nodups_wids\n",
    "    \n",
    "    return(df_newids)\n",
    "\n",
    "def assign_unique_gp2clinicalids(df, clinicalid_subset):\n",
    "\n",
    "    if isinstance(clinicalid_subset, pd.Series):\n",
    "        clinicalid_subset = clinicalid_subset.to_frame().T\n",
    "\n",
    "    sampleid = clinicalid_subset.sort_values(by=['master_GP2sampleID'])\\\n",
    "                                .reset_index(drop = True)\\\n",
    "                                .dropna(subset=['master_GP2sampleID'], axis = 0)\n",
    "    sampleid = sampleid.loc[sampleid.index[-1], 'master_GP2sampleID'].split(\"_\")\n",
    "    getuniqueid = sampleid[0] + \"_\" + sampleid[1]\n",
    "    get_sidrepno = int(sampleid[2].replace(\"s\",\"\")) + 1\n",
    "\n",
    "    index_modify = clinicalid_subset['index'].unique() #clinicalid_subset[clinicalid_subset['GP2sampleID'].isnull()] #.index\n",
    "    assign_gp2sampleid = [getuniqueid + \"_s\" + str(get_sidrepno + i) for i in range(len(index_modify))]\n",
    "    df.loc[index_modify, 'GP2sampleID'] = assign_gp2sampleid\n",
    "    getnewidrows = df.loc[index_modify].copy()\n",
    "    return (getnewidrows)\n",
    "\n",
    "def master_keyv2(studies):\n",
    "    # ACCESS MASTERGP2IDS_JSON IN GP2 BUCKET\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket('eu-samplemanifest')\n",
    "    blob = bucket.blob('IDSTRACKER/GP2IDSMAPPER.json')\n",
    "    \n",
    "    ids_tracker = {}\n",
    "    with blob.open(\"r\") as f:\n",
    "        for k, v in ijson.kvitems(f, ''):\n",
    "            if k in studies:\n",
    "                ids_tracker.update({k:v})\n",
    "    \n",
    "    return(ids_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40*\"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # GENERATE GP2 IDs #\n",
    "        jumptwice()\n",
    "        st.subheader('GP2 IDs assignment...')\n",
    "        studynames = list(df['study'].unique())\n",
    "\n",
    "        if st.session_state['master_get'] == None: # TO ONLY RUN ONCE\n",
    "            #ids_tracker = generategp2ids.master_key(studies = studynames)\n",
    "            ids_tracker = generategp2ids.master_keyv2(studies = studynames)\n",
    "            study_subsets = []\n",
    "            log_new = []\n",
    "            df['GP2sampleID'] = None\n",
    "            # GP2 ID ASSIGNMENT CODE BLOCK\n",
    "            for study in studynames:\n",
    "                st.write(f\"Getting GP2IDs for {study} samples\")\n",
    "                df_subset = df[df.study==study].copy()\n",
    "                try:\n",
    "                    #study_tracker = st.session_state['store_tracker'][study]\n",
    "                    study_tracker = ids_tracker[study]\n",
    "                    study_tracker_df = pd.DataFrame.from_dict(study_tracker,\n",
    "                                                            orient='index',\n",
    "                                                            columns = ['master_GP2sampleID','clinical_id'])\\\n",
    "                                                    .rename_axis('master_sample_id').reset_index()\\\n",
    "                                                    .astype(str)\n",
    "\n",
    "                    # Check if any sample ID exists in df_subset.\n",
    "                    sample_id_unique = pd.merge(study_tracker_df, df_subset,\n",
    "                                                left_on=['master_sample_id'], right_on=['sample_id'], how='inner')\n",
    "                    if not sample_id_unique.empty:\n",
    "                        st.error('We have detected sample ids submitted on previous versions')\n",
    "                        st.error('Please, correct these sample IDs so that they are unique and resubmit the sample manifest.')\n",
    "                        sample_id_unique = sample_id_unique.rename(columns={\"clinical_id_y\": \"clinical_id\"})\n",
    "                        st.dataframe(\n",
    "                        sample_id_unique[['study','sample_id','clinical_id']].style.set_properties(**{\"background-color\": \"brown\", \"color\": \"lawngreen\"})\n",
    "                        )\n",
    "                        stopapp=True\n",
    "                    else:\n",
    "                        stopapp=False\n",
    "                except:\n",
    "                    study_tracker = None\n",
    "                    stopapp = False\n",
    "                if stopapp:\n",
    "                    st.stop()\n",
    "\n",
    "                if bool(study_tracker):\n",
    "                    # WORK ON DUPLICATED IDS\n",
    "                    df_subset = df_subset.reset_index()\n",
    "                    data_duplicated = pd.merge(df_subset, study_tracker_df, on=['clinical_id'], how='inner')\n",
    "                    df_subset = df_subset.set_index('index')\n",
    "                    df_subset.index.name = None\n",
    "\n",
    "                    if data_duplicated.shape[0]>0:\n",
    "                        new_clinicaldups = True\n",
    "                        newids_clinicaldups = data_duplicated.groupby('clinical_id')\\\n",
    "                                                        .apply(lambda x: generategp2ids.assign_unique_gp2clinicalids(df_subset,x))\n",
    "\n",
    "                        if newids_clinicaldups.shape[0]>0:\n",
    "                            newids_clinicaldups = newids_clinicaldups.reset_index(drop=True)[['study','clinical_id','sample_id','GP2sampleID']]\n",
    "                            log_new.append(newids_clinicaldups)\n",
    "                    else:\n",
    "                        new_clinicaldups = False\n",
    "                        newids_clinicaldups = pd.DataFrame()\n",
    "\n",
    "                    # GET GP2 IDs METADATA for new CLINICAL-SAMPLE ID pairs\n",
    "                    df_newids = df_subset[df_subset['GP2sampleID'].isnull()].reset_index(drop = True).copy()\n",
    "                    if not df_newids.empty: # Get new GP2 IDs\n",
    "                        df_wids = df_subset[~df_subset['GP2sampleID'].isnull()].reset_index(drop = True).copy()\n",
    "                        df_wids['GP2ID'] = df_wids['GP2sampleID'].apply(lambda x: (\"_\").join(x.split(\"_\")[:-1]))\n",
    "                        df_wids['SampleRepNo'] = df_wids['GP2sampleID'].apply(lambda x: x.split(\"_\")[-1])#.replace(\"s\",\"\"))\n",
    "\n",
    "                        n=int(max(study_tracker_df['master_GP2sampleID'].to_list()).split(\"_\")[1])+1\n",
    "                        df_newids = generategp2ids.getgp2idsv2(df_newids, n, study)\n",
    "                        df_subset = pd.concat([df_newids, df_wids], axis = 0)\n",
    "                        study_subsets.append(df_subset)\n",
    "                        log_new.append(df_newids[['study','clinical_id','sample_id','GP2sampleID']])\n",
    "                        \n",
    "                    else: # TO CONSIDER THE CASE IN WHICH WE ONLY HAD DUPLICATE IDS MAPPED ON THE MASTER FILE\n",
    "                        df_subset['GP2ID'] = df_subset['GP2sampleID'].apply(lambda x: (\"_\").join(x.split(\"_\")[:-1]))\n",
    "                        df_subset['SampleRepNo'] = df_subset['GP2sampleID'].apply(lambda x: x.split(\"_\")[-1])#.replace(\"s\",\"\"))\n",
    "                        study_subsets.append(df_subset)\n",
    "\n",
    "                # Brand new data - NO STUDY TRACKER FOR THIS COHORT\n",
    "                else:\n",
    "                    study = study\n",
    "                    new_clinicaldups = False # Duplicates from master key json are treated differently to brand new data\n",
    "                    n = 1\n",
    "                    df_newids = generategp2ids.getgp2idsv2(df_subset, n, study)\n",
    "                    study_subsets.append(df_newids)\n",
    "\n",
    "\n",
    "                # CODE TO UPDATE THE GET FILE WE WILL USE TO UPDATE MASTER JSON\n",
    "                if (new_clinicaldups) and (newids_clinicaldups.shape[0]>0):\n",
    "                    tmp = pd.concat([df_newids[['study','clinical_id','sample_id','GP2sampleID']], newids_clinicaldups])\n",
    "                    tmp['master_value'] = list(zip(tmp['GP2sampleID'],\n",
    "                                                    tmp['clinical_id']))\n",
    "                    ids_log = tmp.groupby('study').apply(lambda x: dict(zip(x['sample_id'],\n",
    "                                                                            x['master_value']))).to_dict()\n",
    "                else:\n",
    "                    df_update_master = df_newids.copy()\n",
    "                    df_update_master['master_value'] = list(zip(df_update_master['GP2sampleID'],\n",
    "                                                            df_update_master['clinical_id']))\n",
    "                    ids_log = df_update_master.groupby('study').apply(lambda x: dict(zip(x['sample_id'],\n",
    "                                                                                    x['master_value']))).to_dict()\n",
    "\n",
    "                #generategp2ids.update_masterids(ids_log, study_tracker) # THIS WILL BE UPDATED ONCE THE USET CONFIRMS THE QC ( AT THE END)\n",
    "                \n",
    "                #if st.session_state['master_get'] == None:\n",
    "                if (isinstance(st.session_state['all_ids'], list)):\n",
    "                    st.session_state['all_ids'].append( [ids_log, study_tracker] )\n",
    "                if st.session_state['all_ids'] == None:\n",
    "                    st.session_state['all_ids'] = [ [ids_log, study_tracker] ]\n",
    "            \n",
    "\n",
    "            # OUT OF FOR LOOP // END OF GP2 IDS ASSIGNMENT. LET'S RESUME df.\n",
    "            df = pd.concat(study_subsets, axis = 0)\n",
    "            df = df[list(df)[-3:] + list(df)[:-3]]\n",
    "            st.write(\"GPS IDs assignment... OK\")\n",
    "\n",
    "            #if st.session_state['master_get'] == None:\n",
    "            st.session_state['df_copy'] = df\n",
    "            if len(log_new) > 0:\n",
    "                allnew = pd.concat(log_new, axis = 0).reset_index(drop=True)\n",
    "                st.write(\"Thanks for uploading a new version of the sample manifest\")\n",
    "                st.write(f'We have detected a total of {allnew.shape[0]} new samples')\n",
    "                st.write(\"We have assigned new GP2IDs to those. Showing them below...\")\n",
    "                st.dataframe(\n",
    "                allnew.style.set_properties(**{\"background-color\": \"brown\", \"color\": \"lawngreen\"})\n",
    "                #allnew.style.set_properties(**{\"background-color\": \"brown\", \"color\": \"lawngreen\"})\n",
    "                )\n",
    "            else:\n",
    "                aggridPlotter(df)\n",
    "\n",
    "            st.session_state['df_finalids'] = df\n",
    "            st.session_state['master_get'] = 'DONE'\n",
    "\n",
    "        else:\n",
    "            df = st.session_state['df_finalids']\n",
    "            aggridPlotter(df)\n",
    "            # df_builder = GridOptionsBuilder.from_dataframe(st.session_state['df_copy'])\n",
    "            # df_builder.configure_grid_options(alwaysShowHorizontalScroll = True,\n",
    "            #                                     enableRangeSelection=True,\n",
    "            #                                     pagination=True,\n",
    "            #                                     paginationPageSize=10000,\n",
    "            #                                     domLayout='normal')\n",
    "            # godf = df_builder.build()\n",
    "            # AgGrid(st.session_state['df_copy'],gridOptions=godf, theme='streamlit', height=300)\n",
    "            #df = st.session_state['df_finalids']\n",
    "        #st.session_state['master_get'] = 'DONE'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scverse10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
