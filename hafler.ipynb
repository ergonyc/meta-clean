{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASAP CRN Metadata validation - wave 1\n",
    "\n",
    "# Team Hafler. ASAP CRN Metadata validation - wave 1\n",
    "\n",
    "Teams Hardy, Hafler, and Lee need some minor rationalization.\n",
    "\n",
    "\n",
    "28 Nov 2023\n",
    "Andy Henrie\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit NOT successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.io import read_meta_table, get_dtypes_dict, ReportCollector\n",
    "from utils.qcutils import prep_table, validate_table, validate_file, read_ASAP_CDE, reorder_table_to_CDE\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean V1 Table\n",
    "write clean metadata tables according to CDE v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Hafler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ergonyc/Projects/ASAP/team-hafler/metadata')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert to seurat Object\n",
    "data_path = Path.home() / (\"Projects/ASAP\")\n",
    "metadata_path = data_path / \"team-hafler/metadata\"\n",
    "\n",
    "sheets = [\"SAMPLE\",\"SUBJECT\",\"CLINPATH\",\"STUDY\",\"PROTOCOL\"]\n",
    "excel_path = metadata_path / \"ASAP_CDE_ALL_Team_Hafler_v1.xlsx\"\n",
    "STUDY = pd.read_excel(excel_path,sheet_name=\"STUDY\",header=1).drop(columns=\"Field\")\n",
    "CLINPATH = pd.read_excel(excel_path,sheet_name=\"CLINPATH\",header=1).drop(columns=\"Field\")\n",
    "SUBJECT = pd.read_excel(excel_path,sheet_name=\"SUBJECT\",header=1).drop(columns=\"Field\")\n",
    "SAMPLE = pd.read_excel(excel_path,sheet_name=\"SAMPLE\",header=1).drop(columns=\"Field\")\n",
    "PROTOCOL = pd.read_excel(excel_path,sheet_name=\"PROTOCOL\",header=1).drop(columns=\"Field\")\n",
    "metadata_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read url\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metadata_version = \"v1\"\n",
    "\n",
    "CDE_df,dtypes_dict = read_ASAP_CDE(metadata_version=metadata_version)\n",
    "METADATA_VERSION_DATE = f\"{metadata_version}_{pd.Timestamp.now().strftime('%Y%m%d')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_hafler_batch(sample_df):\n",
    "\n",
    "    # First batch: HSDG07HC HSDG10HC HSDG148PD HSDG199PD\n",
    "    # batch[batch.sample_id in ['hSDG07HC', 'hSDG10HC', 'hSDG148PD', 'hSDG199PD']]=1\n",
    "    Batch_1 = ['hSDG07', 'hSDG10', 'hSDG148', 'hSDG199'] \n",
    "    # Second batch: hsDG101HC hsDG13HC hsDG151PD hsDG197PD hsDG30HC hsDG99HC\n",
    "    Batch_2 = ['hSDG101', 'hSDG13', 'hSDG151', 'hSDG197', 'hSDG30', 'hSDG99']\n",
    "    # Third batch: hsDG142PD hsDG208PD\n",
    "    Batch_3 = ['hSDG142', 'hSDG208'] \n",
    "\n",
    "\n",
    "    batch_col = []\n",
    "    for row in sample_df.sample_id:\n",
    "        if row in Batch_1:\n",
    "            batch_col.append(\"Batch_1\")\n",
    "        elif row in Batch_2:\n",
    "            batch_col.append(\"Batch_2\")\n",
    "        elif row in Batch_3:\n",
    "            batch_col.append(\"Batch_3\")\n",
    "        else:\n",
    "            print(\"ERROR >>>>>>>> not no batch info\")\n",
    "            batch_col.append(\"\")\n",
    "\n",
    "\n",
    "    sample_df['batch'] = batch_col\n",
    "    return sample_df\n",
    "\n",
    "SAMPLE = add_hafler_batch(SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>source_sample_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>replicate</th>\n",
       "      <th>replicate_count</th>\n",
       "      <th>repeated_sample</th>\n",
       "      <th>batch</th>\n",
       "      <th>tissue</th>\n",
       "      <th>brain_region</th>\n",
       "      <th>source_RIN</th>\n",
       "      <th>...</th>\n",
       "      <th>sex_ontology_term_id</th>\n",
       "      <th>self_reported_ethnicity_ontology_term_id</th>\n",
       "      <th>disease_ontology_term_id</th>\n",
       "      <th>tissue_ontology_term_id</th>\n",
       "      <th>cell_type_ontology_term_id</th>\n",
       "      <th>assay_ontology_term_id</th>\n",
       "      <th>suspension_type</th>\n",
       "      <th>DV2000</th>\n",
       "      <th>pm_PH</th>\n",
       "      <th>donor_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hSDG07</td>\n",
       "      <td>hSDG07</td>\n",
       "      <td>HC01</td>\n",
       "      <td>Rep1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Batch_1</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Prefrontal Cortex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hSDG07</td>\n",
       "      <td>hSDG07</td>\n",
       "      <td>HC01</td>\n",
       "      <td>Rep1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Batch_1</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Prefrontal Cortex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hSDG07</td>\n",
       "      <td>hSDG07</td>\n",
       "      <td>HC01</td>\n",
       "      <td>Rep1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Batch_1</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Prefrontal Cortex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hSDG101</td>\n",
       "      <td>hSDG101</td>\n",
       "      <td>HC03</td>\n",
       "      <td>Rep1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Batch_2</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Prefrontal Cortex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hSDG101</td>\n",
       "      <td>hSDG101</td>\n",
       "      <td>HC03</td>\n",
       "      <td>Rep1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Batch_2</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Prefrontal Cortex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id source_sample_id subject_id replicate  replicate_count  \\\n",
       "0    hSDG07           hSDG07       HC01      Rep1                1   \n",
       "1    hSDG07           hSDG07       HC01      Rep1                1   \n",
       "2    hSDG07           hSDG07       HC01      Rep1                1   \n",
       "3   hSDG101          hSDG101       HC03      Rep1                1   \n",
       "4   hSDG101          hSDG101       HC03      Rep1                1   \n",
       "\n",
       "   repeated_sample    batch tissue       brain_region  source_RIN  ...  \\\n",
       "0                0  Batch_1  Brain  Prefrontal Cortex         NaN  ...   \n",
       "1                0  Batch_1  Brain  Prefrontal Cortex         NaN  ...   \n",
       "2                0  Batch_1  Brain  Prefrontal Cortex         NaN  ...   \n",
       "3                0  Batch_2  Brain  Prefrontal Cortex         NaN  ...   \n",
       "4                0  Batch_2  Brain  Prefrontal Cortex         NaN  ...   \n",
       "\n",
       "   sex_ontology_term_id self_reported_ethnicity_ontology_term_id  \\\n",
       "0                   NaN                                      NaN   \n",
       "1                   NaN                                      NaN   \n",
       "2                   NaN                                      NaN   \n",
       "3                   NaN                                      NaN   \n",
       "4                   NaN                                      NaN   \n",
       "\n",
       "  disease_ontology_term_id tissue_ontology_term_id cell_type_ontology_term_id  \\\n",
       "0                      NaN                     NaN                        NaN   \n",
       "1                      NaN                     NaN                        NaN   \n",
       "2                      NaN                     NaN                        NaN   \n",
       "3                      NaN                     NaN                        NaN   \n",
       "4                      NaN                     NaN                        NaN   \n",
       "\n",
       "  assay_ontology_term_id suspension_type DV2000 pm_PH  donor_id  \n",
       "0                    NaN             NaN    NaN   NaN       NaN  \n",
       "1                    NaN             NaN    NaN   NaN       NaN  \n",
       "2                    NaN             NaN    NaN   NaN       NaN  \n",
       "3                    NaN             NaN    NaN   NaN       NaN  \n",
       "4                    NaN             NaN    NaN   NaN       NaN  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix replicate & replicate_count\n",
    "SAMPLE.replace(\"Nan\", \"\", inplace=True)\n",
    "\n",
    "SAMPLE['replicate'] = \"Rep1\"\n",
    "SAMPLE['replicate_count'] = 1\n",
    "\n",
    "SAMPLE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    V1_08/27/2023\n",
       "Name: metadata_version_date, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "STUDY = prep_table(STUDY, CDE_df)\n",
    "STUDY['metadata_version_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update metadata_version_date\n",
    "STUDY['metadata_version_date'] = METADATA_VERSION_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨⚠️❗ **Missing Required Fields in STUDY: submitter_email**\n",
      "No empty entries (Nan) found in _Required_ fields.\n",
      "No empty entries (Nan) found in _Optional_ fields.\n",
      "## Enum fields have valid values in STUDY. 🥳\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(STUDY, \"STUDY\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields are present in *PROTOCOL* table.\n",
      "No empty entries (Nan) found in _Required_ fields.\n",
      "No empty entries (Nan) found in _Optional_ fields.\n",
      "## Enum fields have valid values in PROTOCOL. 🥳\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(PROTOCOL, \"PROTOCOL\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields are present in *SUBJECT* table.\n",
      "No empty entries (Nan) found in _Required_ fields.\n",
      "🚨⚠️❗ **Optional Fields with Empty (nan) values:**\n",
      "\n",
      "\t- primary_diagnosis_text: 12/12 empty rows\n",
      "## Enums\n",
      "🚨⚠️❗ **Invalid entries**\n",
      "- _*sex*_:  invalid values 💩'F', 'M'\n",
      "    - valid ➡️ 'Male', 'Female', 'Intersex', 'Unnown'\n",
      "- _*race*_:  invalid values 💩'B', 'W'\n",
      "    - valid ➡️ 'American Indian or Alaska Native', 'Asian', 'White', 'Black or African American', 'Multi-racial', 'Native Hawaiian or Other Pacific Islander', 'Other', 'Unknown', 'Not Reported'\n",
      "- _*ethnicity*_:  invalid values 💩'B', 'W'\n",
      "    - valid ➡️ 'Hispanic or Latino', 'Not Hispanic or Latino', 'Unknown', 'Not Reported'\n",
      "- _*primary_diagnosis*_:  invalid values 💩'Normal control', 'Idiopathic Parkinson's disease'\n",
      "    - valid ➡️ 'Healthy Control', 'Idiopathic PD', 'Alzheimer’s disease', 'Frontotemporal dementia', 'Corticobasal syndrome', 'Dementia with Lewy bodies', 'Dopa-responsive dystonia', 'Essential tremor', 'Hemiparkinson/hemiatrophy syndrome', 'Juvenile autosomal recessive parkinsonism', 'Motor neuron disease with parkinsonism', 'Multiple system atrophy', 'Neuroleptic-induced parkinsonism', 'Normal pressure hydrocephalus', 'Progressive supranuclear palsy', 'Psychogenic parkinsonism', 'Vascular parkinsonism', 'No PD nor other neurological disorder', 'Spinocerebellar Ataxia (SCA)', 'Prodromal non-motor PD', 'Prodromal motor PD', 'Other neurological disorder'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SUBJECT.replace(\"Nan\", \"\", inplace=True)\n",
    "# Testing the function with SUBJECT.csv and CDE.csv\n",
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(SUBJECT, \"SUBJECT\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SUBJECT = prep_table(SUBJECT, CDE_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields are present in *SUBJECT* table.\n",
      "No empty entries (Nan) found in _Required_ fields.\n",
      "🚨⚠️❗ **Optional Fields with Empty (nan) values:**\n",
      "\n",
      "\t- primary_diagnosis_text: 12/12 empty rows\n",
      "## Enums\n",
      "🚨⚠️❗ **Invalid entries**\n",
      "- _*ethnicity*_:  invalid values 💩'B', 'W'\n",
      "    - valid ➡️ 'Hispanic or Latino', 'Not Hispanic or Latino', 'Unknown', 'Not Reported'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SUBJECT['sex'] = SUBJECT['sex'].replace({'F':\"Female\", 'M':\"Male\"})\n",
    "SUBJECT['race'] = SUBJECT['race'].replace({'W':\"White\", 'B':\"Black or African American\"})\n",
    "\n",
    "SUBJECT['primary_diagnosis'] = SUBJECT['primary_diagnosis'].replace({'Normal control':\"Healthy Control\", \"Idiopathic Parkinson's disease\":\"Idiopathic PD\"})\n",
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(SUBJECT, \"SUBJECT\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they \"race\" was reported under ethnicity, so code as \"Not Reported\"\n",
    "SUBJECT['ethnicity'] = 'Not Reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields are present in *SUBJECT* table.\n",
      "No empty entries (Nan) found in _Required_ fields.\n",
      "🚨⚠️❗ **Optional Fields with Empty (nan) values:**\n",
      "\n",
      "\t- primary_diagnosis_text: 12/12 empty rows\n",
      "## Enum fields have valid values in SUBJECT. 🥳\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(SUBJECT, \"SUBJECT\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = prep_table(SAMPLE,  CDE_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force the right sex_ontology_term_id\n",
    "SAMPLE[\"organism_ontology_term_id\"] = \"NCBITaxon:9606\"\n",
    "\n",
    "# set time == 0 for all samples\n",
    "SAMPLE['time'] = 0\n",
    "\n",
    "SAMPLE['file_type'] = SAMPLE['file_type'].replace({\"Fastq\":\"fastq\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to join with subject to get \"sex\" and convert to ontology term\n",
    "SAMPLE_SUBJECT = SAMPLE.merge(SUBJECT, on='subject_id',  how='left')\n",
    "SAMPLE_og = SAMPLE.copy()\n",
    "SAMPLE['sex_ontology_term_id'] = SAMPLE_SUBJECT['sex'].replace({\"Male\":\"PATO:0000384 (male)\", \"Female\":\"PATO:0000383 (female)\" })\n",
    "\n",
    "# ignore development_stage_ontology_term_id, self_reported_ethnicity_ontology_term_id, assay_ontology_term_id, etc for now. (Check wiht Le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix assay\n",
    "SAMPLE['assay'] = SAMPLE['assay'].replace({'v3.1 - Single Index, 10x Genomics ':\"v3.1 - Single Index\"})\n",
    "# fix assay\n",
    "SAMPLE['sequencing_length'] = SAMPLE['sequencing_length'].replace({'150bp x2':\"150\"})\n",
    "\n",
    "#TODO: add file_description I1, R1, R2  \"Raw sequencing data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields are present in *SAMPLE* table.\n",
      "🚨⚠️❗ **Required Fields with Empty (nan) values:**\n",
      "\n",
      "\t- source_RIN: 36/36 empty rows\n",
      "\n",
      "\t- RIN: 36/36 empty rows\n",
      "\n",
      "\t- suspension_type: 36/36 empty rows\n",
      "🚨⚠️❗ **Optional Fields with Empty (nan) values:**\n",
      "\n",
      "\t- pm_PH: 36/36 empty rows\n",
      "## Enum fields have valid values in SAMPLE. 🥳\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(SAMPLE, \"SAMPLE\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨⚠️❗ **Missing Required Fields in CLINPATH: path_thal**\n",
      "🚨⚠️❗ **Required Fields with Empty (nan) values:**\n",
      "\n",
      "\t- age_at_onset: 12/12 empty rows\n",
      "\n",
      "\t- age_at_diagnosis: 12/12 empty rows\n",
      "\n",
      "\t- first_motor_symptom: 12/12 empty rows\n",
      "\n",
      "\t- path_year_death: 12/12 empty rows\n",
      "\n",
      "\t- brain_weight: 12/12 empty rows\n",
      "🚨⚠️❗ **Optional Fields with Empty (nan) values:**\n",
      "\n",
      "\t- smoking_years: 12/12 empty rows\n",
      "## Enums\n",
      "🚨⚠️❗ **Invalid entries**\n",
      "- _*region_level_2*_:  invalid values 💩'Prefrontal cortex'\n",
      "    - valid ➡️ 'Superior frontal gyrus', 'Middle frontal gyrus', 'Inferior frontal gyrus', 'Superior temporal gyrus', 'Middle temporal gyrus', 'Inferior temporal gyrus', 'Fusiform gyrus', 'Transentorhinal region', 'Entorinal region', 'Subiculum', 'CA1-CA4', 'Amygdala', 'Periamygdala cortex', 'Anterior cingulate gyrus', 'Posterior cingulate gyrus', 'Superior parietal lobule', 'Inferior parietal lobule', 'Parastriate cortex', 'Peristriate cortex', 'Striate cortex', 'Insular cortex', 'Caudate nucleus', 'Putamen', 'Globus pallidus', 'Thalamus', 'Subthalamic nucleus', 'Substantia nigra', 'Pontine tegmentum', 'Pontine base', 'Medulla tegmentum', 'Medulla base', 'Cerebellar vermis', 'Cerebellar hemisphere', 'Dentate nucleus', 'Right colon', 'Left colon', 'Unknown'\n",
      "- _*APOE_e4_status*_:  invalid values 💩'3,3', '2,3'\n",
      "    - valid ➡️ '22', '23', '24', '33', '34', '44', 'Unknown'\n",
      "🚨⚠️❗ **Found unexpected NULL (<NA> or NaN):**\n",
      "- hx_dementia_mcihx_melanoma \n",
      "- education_level \n",
      "- smoking_status \n",
      "- path_autopsy_dx_main \n",
      "- path_braak_nft \n",
      "- path_braak_asyn \n",
      "- path_cerad \n",
      "- known_pathogenic_mutation \n",
      "- path_mckeith \n",
      "- sn_neuronal_loss \n",
      "- path_infarcs \n",
      "- path_nia_ri \n",
      "- path_nia_aa_a \n",
      "- path_nia_aa_b \n",
      "- path_nia_aa_c \n",
      "- TDP43 \n",
      "- arteriolosclerosis_severity_scale \n",
      "- amyloid_angiopathy_severity_scale \n",
      "- path_ad_level\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CLINPATH.replace(\"Nan\", \"\", inplace=True)\n",
    "# CLINPATH.replace(\"nan\", \"\", inplace=True)\n",
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(CLINPATH, \"CLINPATH\", CDE_df, report)\n",
    "report.print_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLINPATH = prep_table(CLINPATH, CDE_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redact \"Prefrontal Cortex\" from region_level_2 for now\n",
    "CLINPATH['region_level_2'] = CLINPATH['region_level_2'].replace({'Prefrontal Cortex':\"\"})\n",
    "\n",
    "# leave te APOE_e4_status as is for now . multiple are coded as \"2,3\" \n",
    "# but remove commas\n",
    "CLINPATH[\"APOE_e4_status\"] = CLINPATH[\"APOE_e4_status\"].str.replace(\",\",\"\")\n",
    "\n",
    "# need to fix the path_autopsy_dx_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨⚠️❗ **Missing Required Fields in CLINPATH: path_thal**\n",
      "🚨⚠️❗ **Required Fields with Empty (nan) values:**\n",
      "\n",
      "\t- age_at_onset: 12/12 empty rows\n",
      "\n",
      "\t- age_at_diagnosis: 12/12 empty rows\n",
      "\n",
      "\t- first_motor_symptom: 12/12 empty rows\n",
      "\n",
      "\t- path_year_death: 12/12 empty rows\n",
      "\n",
      "\t- brain_weight: 12/12 empty rows\n",
      "🚨⚠️❗ **Optional Fields with Empty (nan) values:**\n",
      "\n",
      "\t- smoking_years: 12/12 empty rows\n",
      "## Enums\n",
      "🚨⚠️❗ **Invalid entries**\n",
      "- _*region_level_2*_:  invalid values 💩'Prefrontal cortex'\n",
      "    - valid ➡️ 'Superior frontal gyrus', 'Middle frontal gyrus', 'Inferior frontal gyrus', 'Superior temporal gyrus', 'Middle temporal gyrus', 'Inferior temporal gyrus', 'Fusiform gyrus', 'Transentorhinal region', 'Entorinal region', 'Subiculum', 'CA1-CA4', 'Amygdala', 'Periamygdala cortex', 'Anterior cingulate gyrus', 'Posterior cingulate gyrus', 'Superior parietal lobule', 'Inferior parietal lobule', 'Parastriate cortex', 'Peristriate cortex', 'Striate cortex', 'Insular cortex', 'Caudate nucleus', 'Putamen', 'Globus pallidus', 'Thalamus', 'Subthalamic nucleus', 'Substantia nigra', 'Pontine tegmentum', 'Pontine base', 'Medulla tegmentum', 'Medulla base', 'Cerebellar vermis', 'Cerebellar hemisphere', 'Dentate nucleus', 'Right colon', 'Left colon', 'Unknown'\n",
      "🚨⚠️❗ **Found unexpected NULL (<NA> or NaN):**\n",
      "- hx_dementia_mcihx_melanoma \n",
      "- education_level \n",
      "- smoking_status \n",
      "- path_autopsy_dx_main \n",
      "- path_braak_nft \n",
      "- path_braak_asyn \n",
      "- path_cerad \n",
      "- known_pathogenic_mutation \n",
      "- path_mckeith \n",
      "- sn_neuronal_loss \n",
      "- path_infarcs \n",
      "- path_nia_ri \n",
      "- path_nia_aa_a \n",
      "- path_nia_aa_b \n",
      "- path_nia_aa_c \n",
      "- TDP43 \n",
      "- arteriolosclerosis_severity_scale \n",
      "- amyloid_angiopathy_severity_scale \n",
      "- path_ad_level\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ReportCollector(destination=\"\")\n",
    "validate_table(CLINPATH, \"CLINPATH\", CDE_df, report)\n",
    "report.print_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.home() / \"Projects/ASAP/team-hafler\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added empty column submitter_email to STUDY table\n",
      "added empty column DV200 to SAMPLE table\n",
      "added empty column path_thal to CLINPATH table\n",
      "added empty column dig_slide_avail to CLINPATH table\n",
      "added empty column quant_path_avail to CLINPATH table\n"
     ]
    }
   ],
   "source": [
    "# fix the column order\n",
    "STUDY = reorder_table_to_CDE(STUDY, \"STUDY\", CDE_df)\n",
    "SAMPLE = reorder_table_to_CDE(SAMPLE, \"SAMPLE\", CDE_df)\n",
    "PROTOCOL = reorder_table_to_CDE(PROTOCOL, \"PROTOCOL\", CDE_df)\n",
    "SUBJECT = reorder_table_to_CDE(SUBJECT, \"SUBJECT\", CDE_df)     \n",
    "CLINPATH = reorder_table_to_CDE(CLINPATH, \"CLINPATH\", CDE_df)\n",
    "\n",
    "# write the clean metadata\n",
    "STUDY.to_csv(data_path / \"metadata/STUDY.csv\")\n",
    "PROTOCOL.to_csv(data_path / \"metadata/PROTOCOL.csv\")\n",
    "CLINPATH.to_csv(data_path / \"metadata/CLINPATH.csv\")\n",
    "SAMPLE.to_csv(data_path / \"metadata/SAMPLE.csv\")\n",
    "SUBJECT.to_csv(data_path / \"metadata/SUBJECT.csv\")\n",
    "\n",
    "# also writh them to clean...\n",
    "# \n",
    "#  \n",
    "\n",
    "export_root = Path.cwd() / \"clean/team-Hafler\"\n",
    "if not export_root.exists():\n",
    "    export_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "STUDY.to_csv( export_root / \"STUDY.csv\")\n",
    "PROTOCOL.to_csv(export_root / \"PROTOCOL.csv\")\n",
    "SAMPLE.to_csv(export_root / \"SAMPLE.csv\")\n",
    "SUBJECT.to_csv(export_root / \"SUBJECT.csv\")\n",
    "CLINPATH.to_csv(export_root / \"CLINPATH.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDY.to_csv( metadata_path / \"STUDY.csv\")\n",
    "# CLINPATH.to_csv( metadata_path / \"CLINPATH.csv\")\n",
    "# SUBJECT.to_csv( metadata_path / \"SUBJECT.csv\")\n",
    "# SAMPLE.to_csv( metadata_path / \"SAMPLE.csv\")\n",
    "# PROTOCOL.to_csv( metadata_path / \"PROTOCOL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the table to v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from update_schema import update_tables_to_CDEv2\n",
    "\n",
    "tables_path = Path.cwd() / \"clean/team-Hafler\"\n",
    "\n",
    "\n",
    "CDEv1 = pd.read_csv( Path.cwd() / \"ASAP_CDE_v1.csv\" )\n",
    "CDEv2 = pd.read_csv( Path.cwd() / \"ASAP_CDE_v2.csv\" )\n",
    "\n",
    "\n",
    "STUDYv2, PROTOCOLv2, SAMPLEv2, SUBJECTv2, CLINPATHv2, DATAv2 = update_tables_to_CDEv2(tables_path, CDEv1, CDEv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_root = Path.cwd() / \"clean/team-Hafler/v2\"\n",
    "if not export_root.exists():\n",
    "    export_root.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STUDYv2.to_csv( export_root / \"STUDY.csv\")\n",
    "PROTOCOLv2.to_csv(export_root / \"PROTOCOL.csv\")\n",
    "SAMPLEv2.to_csv(export_root / \"SAMPLE.csv\")\n",
    "SUBJECTv2.to_csv(export_root / \"SUBJECT.csv\")\n",
    "CLINPATHv2.to_csv(export_root / \"CLINPATH.csv\")\n",
    "DATAv2.to_csv(export_root / \"DATA.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer cleaned metadata to raw buckets \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer hafler metadata Hafler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [raw-admin-hafler@dnastack-asap-parkinsons.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "# Lee\n",
    "!gcloud auth activate-service-account --key-file=/Users/ergonyc/Projects/ASAP/hafler-credentials.json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1260  2023-11-30T21:03:06Z  gs://asap-raw-data-team-hafler/metadata/v2/CLINPATH.csv#1701378186816137  metageneration=1\n",
      "      5290  2023-11-30T21:03:07Z  gs://asap-raw-data-team-hafler/metadata/v2/DATA.csv#1701378187386113  metageneration=1\n",
      "      1761  2023-11-30T21:03:07Z  gs://asap-raw-data-team-hafler/metadata/v2/PROTOCOL.csv#1701378187767597  metageneration=1\n",
      "      3260  2023-11-30T21:03:07Z  gs://asap-raw-data-team-hafler/metadata/v2/SAMPLE.csv#1701378187201094  metageneration=1\n",
      "      1446  2023-11-30T21:03:07Z  gs://asap-raw-data-team-hafler/metadata/v2/STUDY.csv#1701378187591914  metageneration=1\n",
      "      1491  2023-11-30T21:03:07Z  gs://asap-raw-data-team-hafler/metadata/v2/SUBJECT.csv#1701378187002956  metageneration=1\n",
      "                                 gs://asap-raw-data-team-hafler/metadata/v2/v2_20231114/\n",
      "                                 gs://asap-raw-data-team-hafler/metadata/v2/v2_20231128/\n",
      "TOTAL: 6 objects, 14508 bytes (14.17 KiB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!gsutil -u dnastack-asap-parkinsons ls -al \"gs://asap-raw-data-team-hafler/metadata/v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ergonyc/Projects/ASAP/meta-clean/hafler.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ergonyc/Projects/ASAP/meta-clean/hafler.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Path\u001b[39m.\u001b[39mcwd()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./clean/team-Hafler/v2_20231130/CLINPATH.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/SUBJECT.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/SAMPLE.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/DATA.csv [Content-Type=text/csv]...\n",
      "- [4 files][ 11.0 KiB/ 11.0 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://./clean/team-Hafler/v2_20231130/STUDY.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/PROTOCOL.csv [Content-Type=text/csv]...\n",
      "- [6 files][ 14.2 KiB/ 14.2 KiB]                                                \n",
      "Operation completed over 6 objects/14.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !gsutil -u dnastack-asap-parkinsons cp -r ./clean/team-Hafler/v2_20231128  \"gs://asap-raw-data-team-hafler/metadata/v2\"\n",
    "!gsutil -u dnastack-asap-parkinsons cp -r \"./clean/team-Hafler/v2_20231130/*.csv\"  \"gs://asap-raw-data-team-hafler/metadata/v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy to workflow-dev bucket\n",
    "\n",
    "First copy each set of metadata locally ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [admin-workflow-dev@dnastack-asap-parkinsons.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth activate-service-account --key-file=/Users/ergonyc/Projects/ASAP/wf-credentials.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./clean/team-Hafler/v2_20231130/CLINPATH.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/SUBJECT.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/SAMPLE.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/DATA.csv [Content-Type=text/csv]...\n",
      "- [4 files][ 11.0 KiB/ 11.0 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://./clean/team-Hafler/v2_20231130/STUDY.csv [Content-Type=text/csv]...\n",
      "Copying file://./clean/team-Hafler/v2_20231130/PROTOCOL.csv [Content-Type=text/csv]...\n",
      "\\ [6 files][ 14.2 KiB/ 14.2 KiB]                                                \n",
      "Operation completed over 6 objects/14.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil  cp -r \"./clean/team-Hafler/v2_20231130/*.csv\"  \"gs://asap-workflow-dev/metadata/v2/hafler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "damn\n"
     ]
    }
   ],
   "source": [
    "print(\"damn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check file md5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.checksums import extract_md5_from_details, extract_md5_from_details2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [raw-admin-hafler@dnastack-asap-parkinsons.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth activate-service-account --key-file=/Users/ergonyc/Projects/ASAP/hafler-credentials.json  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You have requested checksumming but your crcmod installation isn't\n",
      "using the module's C extension, so checksumming will run very slowly. For help\n",
      "installing the extension, please see \"gsutil help crcmod\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !gcloud storage hash \"gs://asap-raw-data-team-hardy/**/*.gz\"  --skip-crc32c --hex  --billing-project dnastack-asap-parkinsons > hardy_hexhash.log\n",
    "\n",
    "!gsutil -u dnastack-asap-parkinsons hash -h \"gs://asap-raw-data-team-hafler/**/*.gz\" > hafler_hexhash.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/hvx6j8cn6yj3bqbyq6rtsxm40000gn/T/ipykernel_56418/1240943610.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  checksum['check1'] =checksum['file_MD5'].str.strip()\n",
      "/var/folders/0z/hvx6j8cn6yj3bqbyq6rtsxm40000gn/T/ipykernel_56418/1240943610.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  checksum['check2'] = checksum['file_name'].map(bucket_files_md5)\n"
     ]
    }
   ],
   "source": [
    "bucket_files_md5 = extract_md5_from_details2(\"hafler_hexhash.log\")\n",
    "\n",
    "\n",
    "\n",
    "checksum = DATAv2[['file_name','file_MD5']]\n",
    "checksum['check1'] = checksum['file_MD5'].str.strip()\n",
    "checksum['check2'] = checksum['file_name'].map(bucket_files_md5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checksum[checksum.check1 != checksum.check2].file_name.to_list()\n",
    "#empty means success!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scverse10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
